{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This reads CSV a given CSV and stores the data in a list\n",
    "def read_csv(data_path):\n",
    "    file_reader = csv.reader(open(data_path,\"rt\", errors=\"ignore\",encoding=\"utf-8\"), delimiter=',')\n",
    "    sent_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for row in file_reader:\n",
    "        id = row[0]\n",
    "        sent = row[1]\n",
    "        sent_list.append((id,sent))\n",
    "        label_list.append(row[2])\n",
    "    return sent_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class taggingParsing:\n",
    "\n",
    "    def sentenceSplit(self, text):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sentences = tokenizer.tokenize(text)\n",
    "        return sentences\n",
    "\n",
    "    def taggingNLTK(self, text):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sentences = tokenizer.tokenize(text)\n",
    "        for sent in sentences:\n",
    "            text = word_tokenize(sent)\n",
    "            tagged_sent = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sent_list):\n",
    "\n",
    "    keywords = [\"suggest\",\"recommend\",\"hopefully\",\"go for\",\"request\",\"it would be nice\",\"adding\",\"should come with\",\"should be able\",\"could come with\", \"i need\" , \"we need\",\"needs\", \"would like to\",\"would love to\",\"allow\",\"add\"]\n",
    "\n",
    "    # Goldberg et al.\n",
    "    pattern_strings = [r'.*would\\slike.*if.*', r'.*i\\swish.*', r'.*i\\shope.*', r'.*i\\swant.*', r'.*hopefully.*',\n",
    "                       r\".*if\\sonly.*\", r\".*would\\sbe\\sbetter\\sif.*\", r\".*should.*\", r\".*would\\sthat.*\",\n",
    "                       r\".*can't\\sbelieve.*didn't.*\", r\".*don't\\sbelieve.*didn't.*\", r\".*do\\swant.*\", r\".*i\\scan\\shas.*\"]\n",
    "    compiled_patterns = []\n",
    "    for patt in pattern_strings:\n",
    "        compiled_patterns.append(re.compile(patt))\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    for sent in sent_list:\n",
    "        tokenized_sent = word_tokenize(sent[1])\n",
    "        tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "        tags = [i[1] for i in tagged_sent]\n",
    "        label = 0\n",
    "        patt_matched = False\n",
    "        for compiled_patt in compiled_patterns:\n",
    "            joined_sent = \" \".join(tokenized_sent)\n",
    "            matches = compiled_patt.findall(joined_sent)\n",
    "            if len(matches) > 0:\n",
    "                patt_matched = True\n",
    "        keyword_match = any(elem in keywords for elem in tokenized_sent)\n",
    "        \n",
    "        \n",
    "        pos_match = any(elem in ['MD', 'VB'] for elem in tags)\n",
    "\n",
    "    \n",
    "\n",
    "        if patt_matched:\n",
    "            label = 1\n",
    "        elif keyword_match == True:\n",
    "                label = 1\n",
    "        elif pos_match == True:\n",
    "                label = 1    \n",
    "     \n",
    "\n",
    "        label_list.append(label)\n",
    "\n",
    "\n",
    "\n",
    "    return label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list,label_list = read_csv('V1.4_Training.csv')\n",
    "label_list = [int(label) for label in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = classify(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.400000000000006\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(sent_list)):\n",
    "    if label_list[i] == predicted_label_list[i]:\n",
    "        count+=1\n",
    "print((float(count)/len(sent_list))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4539"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 0\n",
    "c2 = 0\n",
    "for i in range(len(label_list)):\n",
    "    if label_list[i] == 0:\n",
    "        c1+=1\n",
    "    else:\n",
    "        c2+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6415, 2085)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model = loadGloveModel('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43396 ,  0.73992 ,  0.78403 , -0.41921 ,  0.47901 , -0.90307 ,\n",
       "        0.13821 ,  0.6004  , -0.18415 , -0.62068 , -0.67484 ,  1.2673  ,\n",
       "        0.52699 , -0.11892 ,  0.9452  ,  0.48304 , -0.49922 , -0.98791 ,\n",
       "        0.97262 , -0.98219 ,  0.41451 ,  0.076195,  0.76523 ,  1.1183  ,\n",
       "        1.1039  , -1.2832  , -1.0954  , -0.68419 ,  0.79478 , -1.112   ,\n",
       "        1.8908  ,  0.79092 , -0.74726 ,  0.045625, -0.44904 , -0.26887 ,\n",
       "        0.7335  , -1.1762  ,  0.20768 , -0.45872 ,  1.1217  ,  0.24514 ,\n",
       "        0.39667 ,  0.28376 ,  0.40472 ,  0.6518  ,  0.25236 ,  0.048414,\n",
       "        0.3273  ,  1.1465  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['please']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35145  , -0.24155  ,  0.0054776, -0.43396  ,  0.498    ,\n",
       "       -0.15624  ,  0.085152 ,  0.037574 , -0.08182  , -0.11312  ,\n",
       "        0.30311  ,  0.71108  , -0.18012  , -0.14026  ,  0.72316  ,\n",
       "        1.1194   ,  0.54095  , -0.44946  ,  0.64814  , -0.86225  ,\n",
       "       -0.088763 , -0.055229 ,  0.49666  , -0.14049  ,  0.20234  ,\n",
       "       -2.1223   , -0.061711 , -0.18884  ,  0.15737  , -0.52156  ,\n",
       "        3.7028   ,  0.73726  , -1.0739   , -0.63594  , -0.18347  ,\n",
       "       -0.46252  ,  0.36886  ,  0.19455  , -0.068823 , -0.32577  ,\n",
       "       -0.46426  , -0.096529 ,  0.41884  ,  0.53723  , -0.065486 ,\n",
       "        0.14923  , -0.48415  ,  0.46327  , -0.029425 ,  0.34362  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['should']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sent_list, ):\n",
    "\n",
    "    keywords = [\"suggest\",\"recommend\",\"hopefully\",\"go\",\"request\",\n",
    "                \"would\",\"be\", \"nice\",\"adding\",\"should\", \"come\",\"able\",\n",
    "                \"could\", \"need\",\"needs\", \"like\",\n",
    "                \"love\",\"allow\",\"add\", \"wish\", \"hope\", \"want\", \"hopefully\", \"better\", \"believe\",\n",
    "                \"if\", \"only\", \"didn't\", \"can\", \"can't\"]\n",
    "\n",
    "    # Goldberg et al.\n",
    "    pattern_strings = [r'.*would\\slike.*if.*', r'.*i\\swish.*', r'.*i\\shope.*', r'.*i\\swant.*', r'.*hopefully.*',\n",
    "                       r\".*if\\sonly.*\", r\".*would\\sbe\\sbetter\\sif.*\", r\".*should.*\", r\".*would\\sthat.*\",\n",
    "                       r\".*can't\\sbelieve.*didn't.*\", r\".*don't\\sbelieve.*didn't.*\", r\".*do\\swant.*\", r\".*i\\scan\\shas.*\"]\n",
    "    compiled_patterns = []\n",
    "    for patt in pattern_strings:\n",
    "        compiled_patterns.append(re.compile(patt))\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    features = []\n",
    "    for sent in sent_list:\n",
    "        sent_vector = np.zeros(shape=(50,))\n",
    "        tokenized_sent = word_tokenize(sent[1])\n",
    "        tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "        tags = [i[1] for i in tagged_sent]\n",
    "        label = 0\n",
    "        patt_matched = False\n",
    "        for compiled_patt in compiled_patterns:\n",
    "            joined_sent = \" \".join(tokenized_sent)\n",
    "            matches = compiled_patt.findall(joined_sent)\n",
    "            if len(matches) > 0:\n",
    "                patt_matched = True\n",
    "        keyword_match = any(elem in keywords for elem in tokenized_sent)\n",
    "        for i in range(len(tokenized_sent)):\n",
    "            elem = tokenized_sent[i]\n",
    "            elem = elem.lower()\n",
    "            tag = tagged_sent[i][1]\n",
    "            if model.get(elem) is not None:\n",
    "                if elem in keywords:\n",
    "                    sent_vector+=0.8*model[elem]\n",
    "                elif tag in ['MD', 'VB']:\n",
    "                    sent_vector+=0.15*model[elem]\n",
    "                else:\n",
    "                    sent_vector+=0.05*model[elem]\n",
    "        features.append(sent_vector)\n",
    "    features = np.array(features)\n",
    "    return features\n",
    "        \n",
    "#         for i in range(len(tokenized_sent)):\n",
    "#             tag = tagged_sent[i][1]\n",
    "#             if tag in ['MD', 'VB']:\n",
    "                \n",
    "            \n",
    "#         pos_match = any(elem in ['MD', 'VB'] for elem in tags)\n",
    "\n",
    "    \n",
    "\n",
    "#         if patt_matched:\n",
    "#             label = 1\n",
    "#         elif keyword_match == True:\n",
    "#                 label = 1\n",
    "#         elif pos_match == True:\n",
    "#                 label = 1    \n",
    "     \n",
    "\n",
    "#         label_list.append(label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list,label_list = read_csv('V1.4_Training.csv')\n",
    "features = get_features(sent_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('features', 'wb') as file:\n",
    "#     pickle.dump(features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features', 'rb') as file:\n",
    "    features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = np.array([int(label) for label in label_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shishir\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = svm.SVC(C=20.0, kernel='rbf',degree=2).fit(features, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.8688235294117647\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = classifier.predict(features)\n",
    "print('train accuracy: ', accuracy_score(label_list, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7385\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for i in range(len(predicted_labels)):\n",
    "    if predicted_labels[i] == label_list[i]:\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_list,test_label_list = read_csv('SubtaskA_Trial_Test_Labeled.csv')\n",
    "test_sent_list = test_sent_list[1:]\n",
    "test_label_list = test_label_list[1:]\n",
    "test_label_list = np.array([int(label) for label in test_label_list])\n",
    "test_features = get_features(test_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_test= classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "425\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sent_list))\n",
    "c=0\n",
    "for i in range(len(predicted_labels_test)):\n",
    "    if predicted_labels_test[i] == test_label_list[i]:\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 296\n"
     ]
    }
   ],
   "source": [
    "c1 = 0\n",
    "c2 = 0\n",
    "for i in range(len(test_label_list)):\n",
    "    if test_label_list[i] == 0:\n",
    "        c1+=1\n",
    "    else:\n",
    "        c2+=1\n",
    "print(c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.89      0.76       296\n",
      "           1       0.83      0.55      0.66       296\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       592\n",
      "   macro avg       0.75      0.72      0.71       592\n",
      "weighted avg       0.75      0.72      0.71       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_label_list, predicted_labels_test, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_label_list, predicted_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(mat, labels):\n",
    "    df_cm = pd.DataFrame(mat, index = labels,\n",
    "                  columns = labels)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm, ['0','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7179054054054054\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: ', accuracy_score(test_label_list, predicted_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shishir\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "weighted_classifier = svm.SVC(C=1.0, kernel='rbf',class_weight={1:3}).fit(features, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.8044705882352942\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = weighted_classifier.predict(features)\n",
    "print('train accuracy: ', accuracy_score(label_list, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.73       296\n",
      "           1       0.73      0.76      0.75       296\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       592\n",
      "   macro avg       0.74      0.74      0.74       592\n",
      "weighted avg       0.74      0.74      0.74       592\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sent_list,test_label_list = read_csv('SubtaskA_Trial_Test_Labeled.csv')\n",
    "test_sent_list = test_sent_list[1:]\n",
    "test_label_list = test_label_list[1:]\n",
    "test_label_list = np.array([int(label) for label in test_label_list])\n",
    "test_features = get_features(test_sent_list)\n",
    "predicted_labels_test= weighted_classifier.predict(test_features)\n",
    "print(classification_report(test_label_list, predicted_labels_test, target_names=['0', '1']))\n",
    "cm = confusion_matrix(test_label_list, predicted_labels_test)\n",
    "plot_confusion_matrix(cm, ['0','1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('accuracy: ', accuracy_score(test_label_list, predicted_labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LSTM IMPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_list,label_list\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "# get rid of punctuation\n",
    "# reviews = reviews.lower() # lowercase, standardize\n",
    "sent_list_1 = [sent[1].lower() for sent in sent_list]\n",
    "\n",
    "for i in range(len(sent_list_1)):\n",
    "    sent = sent_list_1[i]\n",
    "    sent = ''.join([c for c in sent if c not in punctuation])\n",
    "    sent_list_1[i] = sent\n",
    "    \n",
    "all_text = ' '.join(sent_list_1)\n",
    "\n",
    "# all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "# reviews_split = all_text.split('\\n')\n",
    "# all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "sent_ints = []\n",
    "for sent in sent_list_1:\n",
    "    sent_ints.append([vocab_to_int[word] for word in sent.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  10529\n",
      "\n",
      "Tokenized review: \n",
      " [[45, 353, 1093, 245, 94, 33, 1, 207, 219, 245, 820, 10, 100, 24, 20, 845, 420, 870, 4, 1906, 2669, 4, 20, 574, 13, 800, 2, 1, 54, 75, 8, 730, 283, 1046, 2, 165, 1, 2670, 283, 1046, 43, 6, 457]]\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', sent_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 13\n",
      "Maximum review length: 343\n"
     ]
    }
   ],
   "source": [
    "# outlier review stats\n",
    "sent_lens = Counter([len(x) for x in sent_ints])\n",
    "print(\"Zero-length reviews: {}\".format(sent_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(sent_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n"
     ]
    }
   ],
   "source": [
    "# for ii, sent in enumerate(sent_ints):\n",
    "#     if len(sent) == 0:\n",
    "#         print('dvbd')\n",
    "print(len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  8500\n",
      "Number of reviews after removing outliers:  8487\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(sent_ints))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, sent in enumerate(sent_ints) if len(sent) != 0]\n",
    "# remove 0-length reviews and their labels\n",
    "sent_ints = [sent_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(sent_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    print(features.shape)\n",
    "    \n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        val = np.array(row)[:seq_length]\n",
    "#         print(val.shape)\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "        \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8487, 200)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(sent_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(7638, 200) \n",
      "Validation set: \t(849, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "# test_idx = int(len(remaining_x)*0.5)\n",
    "# val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "# val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "val_x = remaining_x\n",
    "val_y = remaining_y\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "# test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "# test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ..., 1984,    1,   37],\n",
      "        [   0,    0,    0,  ...,    1,  266,  292],\n",
      "        [   0,    0,    0,  ...,  328,    1,  153],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2232,    1,   14],\n",
      "        [   0,    0,    0,  ...,   81,  953, 1037],\n",
      "        [   0,    0,    0,  ...,    1,   14,  329]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(10530, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
